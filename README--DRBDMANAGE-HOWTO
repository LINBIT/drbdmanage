drbdmanage test-drive HowTo
==============================================================================


INSTALLATION
------------------------------------------------------------------------------

Prerequisites:
    DRBD 9 installed
    LVM installed
Control volume configuration steps:
    *  Create a VG named drbdpool
    *  Create a logical volume named drbdctrl (in whatever VG you want) with
       a size of at least 4 MiB
    *  Configure the drbdctrl resource for DRBD
       (see /etc/drbd.d/drbdctrl.res in the last section of this document)
    *  Create the drbdmanage server configuration file
       (see /etc/drbdmanaged.conf in the last section of this document)
    *  Create the configuration file for the drbdmanage storage plugin
       (see /etc/drbdmanaged-lvm.conf in the last section of this document)
    *  Create the /var/lib/drbdmanage directory on all nodes
    *  Start the control volume on all nodes:
       drbdadm up drbdctrl
    *  Zero out the initial blocks of the control volume:
       dd if=/dev/zero of=/dev/drbd0 bs=4096 count=4
Clone the drbdmanage repository:
    git clone git@bk:drbd/PUBLIC-GPL/drbdmanage /root/dmdemo
    (since you have this file, you have possibly done that already)


Starting the drbdmanage server (daemon)
------------------------------------------------------------------------------
Steps:
    *  Start the control volume on all nodes:
       drbdadm up drbdctrl
    *  Make sure that the control volume is connected on all online nodes

Finally, start the drbdmanage server:
    /root/dmdemo/drbdmanaged.py


Configure your nodes for drbdmanage:
------------------------------------------------------------------------------
Steps:
    *  Create entries for all nodes managed by drbdmanage using the
       drbdmanage client:
       cd /root/dmdemo
       ./drbdmanage.py new-node node_1 ip-address-1
       [...]
       ./drbdmanage.py new-node node_n ip-address-n


drbdmanage commands overview
------------------------------------------------------------------------------

drbdmanaged.py nodes
//  Display the list of nodes defined in drbdmanage

drbdmanaged.py resources
drbdmanaged.py volumes
//  Display the list of resources and volumes defined in drbdmanage

drbdmanaged.py assignments
//  Display the list of resources and volumes that have been assigned to
//  nodes, and the current status of the assignment
//  (e.g., whether the volume has been created already)

drbdmanaged.py new-node [ options ] <name> <ip>
  Options:
    --address-family | -a : { ipv4 | ipv6 }
//  Define a node for use with drbdmanage

drbdmanaged.py new-resource [ options ] <name>
  Options:
    --port | -p : <port-number>
//  Define a resource for use with drbdmanage

drbdmanaged.py new-volume [ options ] <name> <size>
  Options:
    --unit | -u : { MB | GB | TB | PB | MiB | GiB | TiB | PiB }
    --minor | -m : <minor-number>
  The default size unit is GiB.
// Add a volume definition to a resource. If the specified resource does not
// exist, it is created automatically.

drbdmanaged.py remove-node [ --quiet | -q ] <node>
// Remove a node from drbdmanage. This will remove all volumes from the
// specified node.

drbdmanaged.py remove-volume [ --quiet | -q ] <resource> <volume-id>
// Remove a volume from drbdmanage. This will remove the volume from all nodes.

drbdmanaged.py remove-resource [ --quiet | -q ] <resource>
// Remove a resource from drbdmanage. This will remove the resource and all
// its volumes from all nodes.

drbdmanaged.py connect <node> <resource>
// On the specified node, connect the specified DRBD resource to its peer
// resources on other nodes.

drbdmanaged.py disconnect <node> <resource>
// On the specified node, disconnect the specified DRBD resource from all
// other nodes.

drbdmanaged.py reconnect <node> <resource>
// On the specified node, reconnect the specified DRBD resource to its peer
// resources on other nodes.

drbdmanaged.py attach <node> <resource> <volume-id>
// On the specified node, attach the backend storage device of the specified
// volume.

drbdmanaged.py detach <node> <resource> <volume-id>
// On the specified node, detach the backend storage device of the specified
// volume.

drbdmanaged.py assign [ options ] <node> <resource>
  Options:
    --client
      make this node a DRBD client only
    --overwrite
      copy this node’s data to all other nodes
    --discard
      discard this node’s data upon connect
    -c | --connect
      connect to peer resources on other nodes
    The following options are mutually exclusive:
      --overwrite and --client
      --overwrite and --discard
// Assign a resource to a node. This will create the resource’s volumes on the
// specified node.

drbdmanaged.py unassign [ options ] <node> <resource>
  Options:
    --quiet | -q disable the safety question
// Remove the assignment of a resource to a node. This will remove the
// resource and all its volumes from the specified node.

drbdmanaged.py deploy <resource> <redundancy-count>
    The redundancy count specifies the number of
    nodes to which the resource should be deployed. It must be at
    least 1 and less than the maximum allowable number of nodes
    in the cluster
// Automatically deploy the volume to the specified number of nodes. The
// target nodes will be automatically selected by the drbdmanage server.

drbdmanaged.py undeploy [ --quiet | -q ] <resource>
// Automatically undeploy a resource and all of its volumes from all nodes.
// Other than with the remove-resource command, the definition of the
// resource and its volumes is kept.

drbdmanaged.py flags <node> <resource> [ flags ]
  flags:
    --reconnect={0|1}
    --updcon={0|1}
    --overwrite={0|1}
    --discard={0|1}
// Manually set flags on the assignment of a resource to a node.

drbdmanaged.py reconfigure
// Reconfigure the drbdmanage server on the local node
// (reread configuration files, etc.)

drbdmanaged.py update-pool
// Determine total and free space on the local node and update the
// drbdmanage database. This is also done automatically upon start of
// the drbdmanage server.

drbdmanaged.py save
// Write the current state to the drbdmanage database. Normally, issuing
// this command is not necessary, as the database is saved automatically
// whenever changes are performed.

drbdmanaged.py load
// Load the current state of the drbdmanage database. Normally, issuing
// this command is not necessary, as changes to the database on
// backendstorage are detected automatically.
// Background replication after a link had failed previously is not
// detected yet, in this case, the command may be used to cause an
// immediate update of the information on the local node.

drbdmanaged.py export <resource>
drbdmanaged.py export *
// Export configuration files for use with drbdadm from the
// drbdmanage database. If the command is issued from the shell with
// the * joker, the * must be quoted (“*”).


Configuration files: 
------------------------------------------------------------------------------


/etc/drbd.d/drbdctrl.res:
resource drbdctrl {
    net {
        cram-hmac-alg sha1;
        shared-secret “<your-shared-secret>”;
    }
    volume 0 {
        device /dev/drbd0 minor 0;
        disk /dev/mapper/<vgname>-drbdctrl;
        meta-disk internal;
    }
    on <node_1> {
        node-id 0;
        address <ipaddress>:<port>;
    }
    on <node_n> {
        node-id <x>;
        address <ipaddress>:<port>;
    }
    connection-mesh {
        hosts <node_1> <node_2> ... <node_n>;
        net {
            protocol C;
        }
    }
}


/etc/drbdmanaged.conf:
# ============================================================
# DRBDMANAGED CONFIGURATION
# ============================================================
# ============================================================
# STORAGE MANAGER CLASS
#
# drbdmanage will use this class to allocate local storage
# for DRBD volumes
# ============================================================
storage-plugin = drbdmanage.storage.lvm.LVM
# ============================================================
# GREATEST VALID NODE ID
#
# Node ids range from 0 to max-node-id. drbdmanage will not
# allocate node ids greater than max-node-id
#
# (This is commonly the number of nodes minus 1)
# ============================================================
max-node-id = 31
# ============================================================
# MINIMUM MINOR NUMBER
#
# drbdmanaged will leave numbers less than the minimum minor
# number free for manual allocation
# ============================================================
min-minor-nr = 10
# ============================================================
# DRBD PORT RANGE
#
# drbdmanage will allocate port numbers in the range from
# min-port-nr to max-port-nr
# ============================================================
min-port-nr = 7700
max-port-nr = 7899
# ============================================================
# Path for drbdadm configuration files (*.res, commonly)
# ============================================================
drbd-conf-path = /var/drbd.d/


/etc/drbdmanaged-lvm.conf:
# ============================================================
# DRBDMANAGE LVM STORAGE PLUGIN CONFIGURATION
# ============================================================
dev-path = /dev/mapper/
volume-group = drbdpool
lvm-path = /sbin

